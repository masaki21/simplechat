import os
import json
import time
import traceback
from typing import List, Dict, Optional

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from transformers import pipeline
from huggingface_hub import login  # ãƒˆãƒ¼ã‚¯ãƒ³èªè¨¼ç”¨

import torch
import nest_asyncio
from pyngrok import ngrok
import uvicorn
from dotenv import load_dotenv  # â† è¿½åŠ 

# --- .envãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()

# --- ãƒˆãƒ¼ã‚¯ãƒ³èªè¨¼ï¼ˆHugging Faceï¼‰ ---
hf_token = os.getenv("HUGGINGFACE_TOKEN")
if hf_token:
    login(hf_token)
else:
    print("âš ï¸ HUGGINGFACE_TOKEN ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

# --- è¨­å®š ---
MODEL_NAME = os.getenv("MODEL_NAME", "distilgpt2")
print(f"ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: {MODEL_NAME}")
model = None

# --- FastAPIã‚¢ãƒ—ãƒªä½œæˆ ---
app = FastAPI(title="FastAPI LLM API", description="Hugging Face Transformersãƒ™ãƒ¼ã‚¹ã®ãƒãƒ£ãƒƒãƒˆAPI", version="1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ãƒ‡ãƒ¼ã‚¿æ§‹é€ å®šç¾© ---
class Message(BaseModel):
    role: str
    content: str

class RequestBody(BaseModel):
    message: str
    conversationHistory: Optional[List[Message]] = []

class ResponseBody(BaseModel):
    success: bool
    response: str
    conversationHistory: List[Message]

# --- ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ ---
def load_model():
    global model
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹: {device}")
    model = pipeline(
        "text-generation",
        model=MODEL_NAME,
        model_kwargs={"torch_dtype": torch.bfloat16} if device == "cuda" else {},
        device=device
    )
    print("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")

# --- ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆå¿œç­”æŠ½å‡ºé–¢æ•° ---
def extract_response(outputs, prompt):
    if outputs and isinstance(outputs, list):
        text = outputs[0]["generated_text"]
        if prompt in text:
            return text.split(prompt)[-1].strip()
        else:
            return text.strip()
    return "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"

# --- APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ ---
@app.get("/")
async def root():
    return {"message": "FastAPI LLM API is running!"}
@app.post("/chat", response_model=ResponseBody)
async def chat(request: RequestBody):
    global model
    try:
        if model is None:
            load_model()

        messages = request.conversationHistory or []
        messages.append({"role": "user", "content": request.message})

        prompt = ""
        for msg in messages:
            prompt += f"{msg['role']}: {msg['content']}\n"
        prompt += "assistant: "

        print("ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:", prompt)

        outputs = model(prompt, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.9)
        assistant_reply = extract_response(outputs, prompt)

        messages.append({"role": "assistant", "content": assistant_reply})

        return {
            "success": True,
            "response": assistant_reply,
            "conversationHistory": messages
        }

    except Exception as e:
        print("ã‚¨ãƒ©ãƒ¼:", str(e))
        traceback.print_exc()
        raise HTTPException(status_code=500, detail="ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å¿œç­”ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ")

# --- èµ·å‹•æ™‚ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ ---
@app.on_event("startup")
def on_startup():
    load_model()

# --- ngrokãƒˆãƒ³ãƒãƒ«èµ·å‹• ---
def run_with_ngrok(port=8000):
    nest_asyncio.apply()
    ngrok_token = os.getenv("NGROK_TOKEN")
    if not ngrok_token:
        print("âš ï¸ NGROK_TOKEN ãŒ .env ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    ngrok.set_auth_token(ngrok_token)
    public_url = ngrok.connect(port).public_url
    print("ğŸš€ å…¬é–‹URL:", public_url)
    print("ğŸ“˜ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ:", public_url + "/docs")
    uvicorn.run(app, host="0.0.0.0", port=port)

# --- å®Ÿè¡Œãƒ–ãƒ­ãƒƒã‚¯ ---
if __name__ == "__main__":
    run_with_ngrok(port=8000)
